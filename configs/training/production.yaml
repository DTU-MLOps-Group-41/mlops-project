defaults:
  - baseline

# Production training configuration
# Inherits from baseline, overrides for production-quality training

batch_size: 32
learning_rate: 5e-5
num_epochs: 10
deterministic: true
log_every_n_steps: 50
patience: 5
save_best_only: false
checkpoint_save_top_k: 3

# Enable advanced training features
gradient_clip_val: 1.0
accumulate_grad_batches: 2

# Learning rate scheduler using Hydra instantiate
lr_scheduler:
  _target_: transformers.get_scheduler
  name: cosine
  num_warmup_steps: 100
  # Runtime parameters (filled by model.py):
  # - optimizer
  # - num_training_steps

# Training metadata
name: production
description: "Production-grade training configuration"
