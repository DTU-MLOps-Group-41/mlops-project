# W&B Sweep Configuration for DistilBERT Fine-tuning
# Optimizes hyperparameters for customer support ticket classification
#
# Usage:
#   wandb sweep configs/sweep.yaml
#   wandb agent <sweep_id> --count 10
#   --count 10 ensures to only run 10 experiments and then stop
#
# Best Practice: Uses val_accuracy (not loss) as the optimization metric.
# Research shows accuracy-based metrics are preferred for fine-tuning because:
# - Loss can be misleading (low loss doesn't guarantee good predictions)
# - Accuracy directly measures classification performance
# - Fine-tuned transformers often have calibration issues with loss

program: src/customer_support/train.py
project: customer_support
entity: dtu-mlops-g41
method: bayes
name: distilbert-finetuning-sweep

# Custom command for Hydra compatibility (Hydra uses key=value, not --key=value)
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - ${args_no_hyphens}

metric:
  name: val_accuracy
  goal: maximize


parameters:
  # Learning rate: most critical hyperparameter for fine-tuning
  # Log-uniform distribution captures order-of-magnitude differences
  training.learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 5e-4

  # Batch size affects training stability and generalization
  training.batch_size:
    values: [32, 64]

  # Weight decay for L2 regularization to prevent overfitting
  training.weight_decay:
    distribution: uniform
    min: 0.0
    max: 0.1


  # Fixed parameters for reproducibility and consistency
  seed:
    value: 42
  dataset:
    value: full
  wandb.mode:
    value: online
  training.num_epochs:
    value: 3
  training.patience:
    value: 2
  training.log_model:
    value: false
  training.enable_checkpointing:
    value: false
