{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Customer Support Ticket Classification","text":"<p>A almost-production-ready MLOps project for automatically classifying customer support ticket priority using DistilBERT.</p>"},{"location":"#overview","title":"Overview","text":"<p>This project implements an end-to-end machine learning pipeline for classifying customer support tickets into three priority levels: low, medium, and high. It demonstrates MLOps best practices including experiment tracking, data versioning, containerized training, and cloud deployment.</p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>Kaggle Dataset\n      |\n      v\nData Preprocessing (tokenization, splitting)\n      |\n      v\nDVC (versioned data on GCS)\n      |\n      v\nTraining (Local or Vertex AI)\n      |\n      v\nExperiment Tracking (Weights &amp; Biases)\n      |\n      v\nModel Checkpoint\n      |\n      v\nFastAPI Inference Service\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>DistilBERT Model - Multilingual transformer for text classification</li> <li>PyTorch Lightning - Scalable training with mixed precision and distributed support</li> <li>Hydra Configuration - Flexible experiment management with config composition</li> <li>Weights &amp; Biases - Experiment tracking and model versioning</li> <li>DVC - Data versioning with Google Cloud Storage backend</li> <li>FastAPI - REST API for real-time inference</li> <li>Docker - Containerized training (CPU/GPU) and serving</li> <li>Vertex AI - Managed cloud training on GCP</li> <li>GitHub Actions - CI/CD with multi-platform testing</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"Topic Description Getting Started Setup guide for new developers Data Processing Dataset details and preprocessing Model Model architecture and configuration Training Local and cloud training guide API Inference service documentation CLI Commands All available invoke tasks Testing Test suite and CI/CD DVC Data versioning workflow Cloud GCP infrastructure and Vertex AI"},{"location":"api/","title":"API Service","text":"<p>The project includes a FastAPI REST service for real-time ticket priority classification.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>The API provides a simple interface for classifying customer support tickets:</p> <ul> <li>Framework: FastAPI with Uvicorn</li> <li>Model: DistilBERT loaded from checkpoint</li> <li>Port: 8080 (default)</li> </ul>"},{"location":"api/#endpoints","title":"Endpoints","text":""},{"location":"api/#get","title":"<code>GET /</code>","text":"<p>Returns API information and available endpoints.</p> <p>Response:</p> <pre><code>{\n  \"service\": \"Customer Support Ticket Classifier\",\n  \"version\": \"1.0.0\",\n  \"endpoints\": {\n    \"predict\": \"POST /predict\",\n    \"health\": \"GET /health\"\n  }\n}\n</code></pre>"},{"location":"api/#get-health","title":"<code>GET /health</code>","text":"<p>Health check endpoint for container orchestration (Kubernetes, Cloud Run, etc.).</p> <p>Response:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"model_loaded\": true\n}\n</code></pre>"},{"location":"api/#post-predict","title":"<code>POST /predict</code>","text":"<p>Classify a ticket's priority from its text.</p> <p>Request Body:</p> <pre><code>{\n  \"text\": \"My computer won't start and I have an important presentation in 1 hour!\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"priority\": \"high\",\n  \"priority_id\": 2,\n  \"confidence\": 0.87\n}\n</code></pre> <p>Fields:</p> Field Type Description <code>priority</code> string Priority level: <code>\"low\"</code>, <code>\"medium\"</code>, or <code>\"high\"</code> <code>priority_id</code> int Numeric class ID: 0, 1, or 2 <code>confidence</code> float Model confidence (probability) <p>Error Responses:</p> <ul> <li><code>422 Validation Error</code> - Missing or empty text field</li> <li><code>503 Service Unavailable</code> - Model checkpoint not found</li> </ul>"},{"location":"api/#running-locally","title":"Running Locally","text":""},{"location":"api/#prerequisites","title":"Prerequisites","text":"<ul> <li>Trained model checkpoint at <code>models/model.ckpt</code> (or set <code>MODEL_PATH</code> env var)</li> </ul>"},{"location":"api/#start-the-server","title":"Start the server","text":"<pre><code># Default: loads model from models/model.ckpt\nuvicorn customer_support.api:app --host 0.0.0.0 --port 8080\n\n# With custom model path\nMODEL_PATH=/path/to/checkpoint.ckpt uvicorn customer_support.api:app --port 8080\n\n# Development mode with auto-reload\nuvicorn customer_support.api:app --reload --port 8080\n</code></pre>"},{"location":"api/#test-the-api","title":"Test the API","text":"<pre><code># Health check\ncurl http://localhost:8080/health\n\n# Classify a ticket\ncurl -X POST http://localhost:8080/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Cannot login to my account, need help urgently\"}'\n</code></pre>"},{"location":"api/#docker-deployment","title":"Docker Deployment","text":""},{"location":"api/#build-the-image","title":"Build the image","text":"<pre><code>docker build -t api:latest -f dockerfiles/api.dockerfile .\n</code></pre>"},{"location":"api/#run-the-container","title":"Run the container","text":"<pre><code>docker run -p 8080:8080 \\\n  -v /path/to/models:/app/models \\\n  -e MODEL_PATH=/app/models/model.ckpt \\\n  api:latest\n</code></pre>"},{"location":"api/#with-gpu-support","title":"With GPU support","text":"<pre><code># Build with CUDA\ndocker build -t api:gpu -f dockerfiles/api.dockerfile --build-arg DEVICE=cu128 .\n\n# Run with GPU\ndocker run --gpus all -p 8080:8080 \\\n  -v /path/to/models:/app/models \\\n  -e MODEL_PATH=/app/models/model.ckpt \\\n  api:gpu\n</code></pre>"},{"location":"api/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>MODEL_PATH</code> <code>models/model.ckpt</code> Path to model checkpoint"},{"location":"api/#api-implementation-details","title":"API Implementation Details","text":"<p>The API is implemented in <code>src/customer_support/api.py</code>:</p> <ul> <li>Model loading: Loaded once at startup via FastAPI lifespan</li> <li>Tokenizer caching: LRU cache for the DistilBERT tokenizer</li> <li>Inference mode: Uses <code>torch.inference_mode()</code> for faster predictions</li> <li>Non-root user: Docker container runs as unprivileged user for security</li> </ul>"},{"location":"api/#example-python-client","title":"Example: Python Client","text":"<pre><code>import requests\n\ndef classify_ticket(text: str, api_url: str = \"http://localhost:8080\") -&gt; dict:\n    response = requests.post(\n        f\"{api_url}/predict\",\n        json={\"text\": text}\n    )\n    response.raise_for_status()\n    return response.json()\n\n# Usage\nresult = classify_ticket(\"My laptop screen is broken\")\nprint(f\"Priority: {result['priority']} (confidence: {result['confidence']:.2%})\")\n</code></pre>"},{"location":"api/#streamlit-web-interface","title":"Streamlit Web Interface","text":"<p>A user-friendly web interface is available through Streamlit, allowing interactive ticket classification with a visual dashboard.</p>"},{"location":"api/#running-the-streamlit-app","title":"Running the Streamlit App","text":"<pre><code># Run with uv\nuv run streamlit run src/customer_support/streamlit_app.py\n\n# Or directly with streamlit\nstreamlit run src/customer_support/streamlit_app.py\n</code></pre> <p>The app will open at <code>http://localhost:8501</code> and provides:</p> <ul> <li>Interactive ticket input - Enter ticket text directly in the browser</li> <li>Real-time classification - Instant priority predictions with confidence scores</li> <li>Visual badges - Color-coded priority indicators (\ud83d\udfe2 Low, \ud83d\udfe1 Medium, \ud83d\udd34 High)</li> <li>Sample tickets - Pre-loaded examples to test the model</li> <li>Prediction history - Track all classifications in the current session</li> <li>Model information - Display model details and documentation links</li> </ul>"},{"location":"api/#features","title":"Features","text":"<ul> <li>No API dependency - The Streamlit app loads the model directly from checkpoint</li> <li>Session state - Maintains prediction history within a session</li> <li>Custom branding - Styled to match project documentation</li> <li>Responsive design - Works on desktop and mobile browsers</li> </ul>"},{"location":"api/#environment-variables_1","title":"Environment Variables","text":"<p>The Streamlit app respects the same <code>MODEL_PATH</code> environment variable as the API:</p> <pre><code>MODEL_PATH=/path/to/checkpoint.ckpt uv run streamlit run src/customer_support/streamlit_app.py\n</code></pre>"},{"location":"api/#related-documentation","title":"Related Documentation","text":"<ul> <li>Model - Model architecture details</li> <li>Cloud - Deploying to GCP</li> </ul>"},{"location":"cli/","title":"CLI Commands","text":"<p>This project uses Invoke for task automation. All commands are defined in <code>tasks.py</code>.</p>"},{"location":"cli/#usage","title":"Usage","text":"<p>Run commands with:</p> <pre><code>invoke &lt;command&gt;\n# or\ninv &lt;command&gt;\n</code></pre> <p>List all available commands:</p> <pre><code>invoke --list\n</code></pre>"},{"location":"cli/#data-commands","title":"Data Commands","text":""},{"location":"cli/#download-data","title":"<code>download-data</code>","text":"<p>Download raw dataset from Kaggle.</p> <pre><code>invoke download-data\n</code></pre> <p>Downloads all dataset sizes (small, medium, full) to <code>data/raw/</code>.</p> <p>Prerequisites: Kaggle credentials configured (setup guide)</p>"},{"location":"cli/#preprocess-data","title":"<code>preprocess-data</code>","text":"<p>Preprocess datasets (clean, tokenize, split, save as Parquet).</p> <pre><code># Process all dataset sizes\ninvoke preprocess-data\n\n# Process a specific size\ninvoke preprocess-data --dataset-type small\ninvoke preprocess-data --dataset-type medium\ninvoke preprocess-data --dataset-type full\n\n# With sequence length filtering\ninvoke preprocess-data --length-percentile 90 --length-handling trim\ninvoke preprocess-data --length-percentile 95 --length-handling drop\n</code></pre> <p>Options:</p> Option Description <code>--dataset-type</code> Dataset size: <code>small</code>, <code>medium</code>, or <code>full</code> <code>--length-percentile</code> Percentile threshold for sequence length (0 = no filtering) <code>--length-handling</code> How to handle long sequences: <code>trim</code> or <code>drop</code>"},{"location":"cli/#training-commands","title":"Training Commands","text":""},{"location":"cli/#train","title":"<code>train</code>","text":"<p>Train the model with default configuration.</p> <pre><code>invoke train\n</code></pre> <p>This runs <code>uv run src/customer_support/train.py</code>. For configuration options, see Training.</p>"},{"location":"cli/#evaluate","title":"<code>evaluate</code>","text":"<p>Evaluate a trained model on the test set.</p> <pre><code>invoke evaluate --checkpoint path/to/model.ckpt\n</code></pre> <p>Required: <code>--checkpoint</code> - Path to model checkpoint file</p>"},{"location":"cli/#visualize","title":"<code>visualize</code>","text":"<p>Generate visualizations (confusion matrix) for a trained model.</p> <pre><code>invoke visualize --checkpoint path/to/model.ckpt\n</code></pre> <p>Required: <code>--checkpoint</code> - Path to model checkpoint file</p>"},{"location":"cli/#testing-commands","title":"Testing Commands","text":""},{"location":"cli/#test","title":"<code>test</code>","text":"<p>Run the test suite with coverage reporting.</p> <pre><code>invoke test\n</code></pre> <p>This runs: 1. <code>uv run coverage run -m pytest tests/</code> 2. <code>uv run coverage report -m -i</code></p>"},{"location":"cli/#docker-commands","title":"Docker Commands","text":""},{"location":"cli/#docker-build","title":"<code>docker-build</code>","text":"<p>Build both API and training Docker images.</p> <pre><code>invoke docker-build\n\n# With verbose progress output\ninvoke docker-build --progress auto\n</code></pre> <p>Options:</p> Option Default Description <code>--progress</code> <code>plain</code> Docker build progress output: <code>plain</code>, <code>auto</code>, <code>tty</code> <p>Images built:</p> <ul> <li><code>train:latest</code> - Training image from <code>dockerfiles/train.dockerfile</code></li> <li><code>api:latest</code> - API image from <code>dockerfiles/api.dockerfile</code></li> </ul>"},{"location":"cli/#documentation-commands","title":"Documentation Commands","text":""},{"location":"cli/#build-docs","title":"<code>build-docs</code>","text":"<p>Build the MkDocs documentation site.</p> <pre><code>invoke build-docs\n</code></pre> <p>Output is written to <code>docs/build/</code>.</p>"},{"location":"cli/#serve-docs","title":"<code>serve-docs</code>","text":"<p>Start the documentation development server.</p> <pre><code>invoke serve-docs\n</code></pre> <p>Opens at http://127.0.0.1:8000 with live reload.</p>"},{"location":"cli/#utility-commands","title":"Utility Commands","text":""},{"location":"cli/#project-tree","title":"<code>project-tree</code>","text":"<p>Generate a tree view of the project structure.</p> <pre><code># Full tree\ninvoke project-tree\n\n# Limit depth\ninvoke project-tree --depth 2\n</code></pre> <p>Options:</p> Option Default Description <code>--depth</code> 0 (unlimited) Maximum directory depth to display"},{"location":"cli/#quick-reference","title":"Quick Reference","text":"Command Description <code>invoke download-data</code> Download Kaggle dataset <code>invoke preprocess-data</code> Preprocess all datasets <code>invoke train</code> Train model <code>invoke evaluate --checkpoint &lt;path&gt;</code> Evaluate model <code>invoke visualize --checkpoint &lt;path&gt;</code> Generate visualizations <code>invoke test</code> Run tests with coverage <code>invoke docker-build</code> Build Docker images <code>invoke build-docs</code> Build documentation <code>invoke serve-docs</code> Serve documentation locally"},{"location":"cloud/","title":"Cloud Infrastructure","text":""},{"location":"cloud/#cloud-infrastructure","title":"Cloud Infrastructure","text":"<p>This project uses Google Cloud Platform (GCP) for cloud-based training. The infrastructure includes Cloud Build for CI/CD, Artifact Registry for Docker images, Vertex AI for managed training, and Google Cloud Storage for data (via DVC).</p>"},{"location":"cloud/#architecture-overview","title":"Architecture Overview","text":"<pre><code>GitHub Repository\n        \u2502\n        \u2502 push to main\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Cloud Build     \u2502 \u25c4\u2500\u2500 Automatic trigger\n\u2502  (Docker images)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Artifact Registry \u2502\n\u2502  europe-west1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u2502 Manual: gcloud builds submit\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Vertex AI      \u2502 \u25c4\u2500\u2500\u2500\u2500\u2502  GCS Bucket   \u2502\n\u2502  Training Jobs    \u2502      \u2502  (DVC data)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Weights &amp; Biases  \u2502\n\u2502 (Experiment logs) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Components:</p> <ul> <li>Artifact Registry: <code>europe-west1-docker.pkg.dev/corded-smithy-484309-s4/g41-registry/</code></li> <li>GCS Data Bucket: <code>gs://my_mlops_data_bucket3/</code></li> <li>WandB: Entity <code>dtu-mlops-g41</code>, Project <code>customer_support</code></li> <li>Region: <code>europe-west1</code></li> </ul>"},{"location":"cloud/#docker-images","title":"Docker Images","text":"<p>Two training images are maintained:</p> Image Base Use Case <code>train-cpu</code> <code>uv:python3.12-bookworm-slim</code> CPU-only training <code>train-cu128</code> <code>nvidia/cuda:12.8.0-runtime-ubuntu24.04</code> GPU training (CUDA 12.8)"},{"location":"cloud/#automatic-builds","title":"Automatic Builds","text":"<p>Docker images are automatically built and pushed when commits are pushed to the <code>main</code> branch. This is configured as a Cloud Build trigger in the GCP Console.</p> <p>The build process (<code>.gcp/cloudbuild.yaml</code>) uses Docker BuildKit and pushes to Artifact Registry.</p>"},{"location":"cloud/#manual-image-build","title":"Manual Image Build","text":"<p>To manually trigger a build:</p> <pre><code># Build GPU image\ngcloud builds submit . --config=.gcp/cloudbuild.yaml \\\n  --substitutions=_DEVICE=cu128,_IMAGE_NAME=train-cu128:latest,_DOCKERFILE=dockerfiles/train_cu128.dockerfile\n\n# Build CPU image\ngcloud builds submit . --config=.gcp/cloudbuild.yaml \\\n  --substitutions=_DEVICE=cpu,_IMAGE_NAME=train-cpu:latest,_DOCKERFILE=dockerfiles/train_cpu.dockerfile\n</code></pre>"},{"location":"cloud/#container-entrypoint","title":"Container Entrypoint","text":"<p>Both images use <code>dockerfiles/entrypoint.sh</code> which:</p> <ol> <li>Configures DVC for containerized environment (<code>core.no_scm true</code>)</li> <li>Pulls training data from GCS via DVC</li> <li>Runs the training script with provided arguments</li> </ol>"},{"location":"cloud/#vertex-ai-training","title":"Vertex AI Training","text":""},{"location":"cloud/#submitting-training-jobs","title":"Submitting Training Jobs","text":"<p>Training jobs are submitted via Cloud Build, which handles secret injection and job creation:</p> <pre><code># GPU training (recommended for production)\ngcloud builds submit . --config=.gcp/vertex_ai_train.yaml \\\n  --substitutions=_VERTEX_TRAIN_CONFIG=.gcp/config_gpu.yaml\n\n# CPU training (for testing)\ngcloud builds submit . --config=.gcp/vertex_ai_train.yaml \\\n  --substitutions=_VERTEX_TRAIN_CONFIG=.gcp/config_cpu.yaml\n</code></pre>"},{"location":"cloud/#training-configurations","title":"Training Configurations","text":"<p>GPU Configuration (<code>.gcp/config_gpu.yaml</code>):</p> <ul> <li>Machine: <code>n1-standard-8</code> with 1x NVIDIA Tesla T4</li> <li>Image: <code>train-cu128:latest</code></li> <li>Default experiment: <code>experiment=best</code> (optimized hyperparameters, full dataset)</li> </ul> <p>CPU Configuration (<code>.gcp/config_cpu.yaml</code>):</p> <ul> <li>Machine: <code>n1-standard-8</code></li> <li>Image: <code>train-cpu:latest</code></li> <li>Default experiment: <code>experiment=baseline</code> (quick testing, small dataset)</li> </ul>"},{"location":"cloud/#customizing-training","title":"Customizing Training","text":"<p>Modify the <code>args</code> in the config files to pass Hydra overrides:</p> <pre><code>args:\n  - \"experiment=best\"\n  - \"training.num_epochs=10\"\n  - \"training.batch_size=32\"\n</code></pre> <p>Available experiments:</p> <ul> <li><code>experiment=baseline</code> - Quick testing on small dataset</li> <li><code>experiment=best</code> - Optimized hyperparameters on full dataset</li> </ul>"},{"location":"cloud/#secrets-management","title":"Secrets Management","text":"<p>The WandB API key is stored in Google Secret Manager and automatically injected during training job submission.</p>"},{"location":"cloud/#experiment-tracking","title":"Experiment Tracking","text":"<p>Training runs are logged to Weights &amp; Biases:</p> <ul> <li>Entity: <code>dtu-mlops-g41</code></li> <li>Project: <code>customer_support</code></li> <li>Dashboard: wandb.ai/dtu-mlops-g41/customer_support</li> </ul> <p>Configuration in <code>configs/config.yaml</code>:</p> <pre><code>wandb:\n  entity: \"dtu-mlops-g41\"\n  project: \"customer_support\"\n  mode: \"online\"\n  log_model: true\n</code></pre> <p>Note: WandB Model Registry integration is not yet configured. Currently <code>log_model: true</code> logs model artifacts to individual runs.</p>"},{"location":"cloud/#prerequisites","title":"Prerequisites","text":""},{"location":"cloud/#gcp-authentication","title":"GCP Authentication","text":"<pre><code>gcloud auth login\ngcloud config set project corded-smithy-484309-s4\ngcloud config set compute/region europe-west1\n</code></pre>"},{"location":"cloud/#required-permissions","title":"Required Permissions","text":"<ul> <li>Cloud Build Editor - submit builds</li> <li>Vertex AI User - create training jobs</li> <li>Storage Object Viewer - access DVC data</li> <li>Secret Manager Secret Accessor - access WANDB_API_KEY (for service account)</li> </ul>"},{"location":"cloud/#monitoring","title":"Monitoring","text":""},{"location":"cloud/#cloud-build","title":"Cloud Build","text":"<pre><code># List recent builds\ngcloud builds list --limit=10\n\n# Stream build logs\ngcloud builds log BUILD_ID --stream\n</code></pre>"},{"location":"cloud/#vertex-ai-jobs","title":"Vertex AI Jobs","text":"<pre><code># List training jobs\ngcloud ai custom-jobs list --region=europe-west1\n\n# View job details\ngcloud ai custom-jobs describe JOB_ID --region=europe-west1\n</code></pre>"},{"location":"cloud/#troubleshooting","title":"Troubleshooting","text":"<p>Build fails with permission errors</p> <ul> <li>Verify Cloud Build service account has Artifact Registry Writer role</li> <li>Check Secret Manager access for WANDB_API_KEY</li> </ul> <p>Training job fails to start</p> <ul> <li>Check GPU quota in europe-west1 region</li> <li>Verify image exists: <code>gcloud artifacts docker images list europe-west1-docker.pkg.dev/corded-smithy-484309-s4/g41-registry</code></li> </ul> <p>DVC pull fails in container</p> <ul> <li>Ensure GCS bucket allows access from Vertex AI service account</li> <li>Check <code>.dvc/config</code> remote URL matches bucket</li> </ul>"},{"location":"data/","title":"Data Processing","text":""},{"location":"data/#data-processing","title":"Data Processing","text":"<p>This project uses the Multilingual Customer Support Tickets dataset from Kaggle for training a ticket priority classification model. The dataset contains 28,600 customer support tickets with descriptions and priority labels.</p> <p>Three dataset sizes are available: - small: ~4,000 tickets (for rapid experimentation) - medium: ~20,000 tickets (for balanced training) - full: ~50,000+ tickets (for maximum performance)</p>"},{"location":"data/#dataset-details","title":"Dataset Details","text":"<p>The dataset consists of multilingual customer support tickets with the following characteristics:</p> <ul> <li>Source: Kaggle - Customer IT Support Ticket Dataset</li> <li>Total observations: 28,600 tickets</li> <li>Features used:</li> <li><code>body</code>: Customer's ticket description (text)</li> <li><code>priority</code>: Ticket priority level (label)</li> <li>Label mapping:</li> <li><code>very_low</code> \u2192 0</li> <li><code>low</code> \u2192 1</li> <li><code>medium</code> \u2192 2</li> <li><code>high</code> \u2192 3</li> <li><code>critical</code> \u2192 4</li> <li>Data splits: 80% train, 10% validation, 10% test (stratified by labels for balanced representation)</li> <li>Tokenization: DistilBERT multilingual model (<code>distilbert-base-multilingual-cased</code>)</li> </ul> <p>Note: The original dataset contains additional fields (topic, department) that are not currently used in the preprocessing pipeline.</p>"},{"location":"data/#preprocessing-pipeline","title":"Preprocessing Pipeline","text":"<p>The automated preprocessing pipeline performs the following steps:</p> <ol> <li>Download: Fetch dataset from Kaggle using KaggleHub (with automatic caching)</li> <li>Clean: Select relevant columns (<code>body</code>, <code>priority</code>) and remove rows with missing values</li> <li>Encode: Convert priority strings to integer labels using the label mapping</li> <li>Tokenize: Apply DistilBERT multilingual tokenizer to ticket body text</li> <li>Split: Divide into train/validation/test sets with stratified sampling</li> <li>Save: Export as Parquet files for efficient loading</li> </ol> <p>All preprocessing is handled automatically by the <code>TicketDataset</code> class or can be run manually via CLI commands.</p>"},{"location":"data/#quick-start","title":"Quick Start","text":""},{"location":"data/#download-raw-data-from-kaggle","title":"Download raw data from Kaggle","text":"<pre><code>uv run src/customer_support/data.py download\n</code></pre> <p>Or using invoke:</p> <pre><code>uv run invoke download-data\n</code></pre>"},{"location":"data/#preprocess-a-dataset","title":"Preprocess a dataset","text":"<pre><code># Preprocess the small dataset\nuv run src/customer_support/data.py preprocess -d small\n\n# Preprocess all datasets\nuv run src/customer_support/data.py preprocess --all\n</code></pre> <p>Or using invoke:</p> <pre><code>uv run invoke preprocess-data --dataset-type small\nuv run invoke preprocess-data  # processes all sizes\n</code></pre>"},{"location":"data/#load-preprocessed-data-in-python","title":"Load preprocessed data in Python","text":"<pre><code>from customer_support.data import TicketDataset\n\n# Load preprocessed training data\ntrain_data = TicketDataset(root=\"data\", split=\"train\", dataset_type=\"small\")\n\n# Access a sample\nsample = train_data[0]\nprint(sample.keys())  # dict_keys(['input_ids', 'attention_mask', 'labels'])\n</code></pre>"},{"location":"data/#use-with-pytorch-dataloader","title":"Use with PyTorch DataLoader","text":"<pre><code>from torch.utils.data import DataLoader\nfrom customer_support.data import TicketDataset\n\ntrain_data = TicketDataset(root=\"data\", split=\"train\", dataset_type=\"small\")\nloader = DataLoader(train_data, batch_size=32, shuffle=True)\n\nfor batch in loader:\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    labels = batch[\"labels\"]\n    # Training logic here\n</code></pre>"},{"location":"data/#cli-commands","title":"CLI Commands","text":"<p>The data module provides two main CLI commands via Typer.</p>"},{"location":"data/#download-command","title":"Download Command","text":"<p>Downloads all dataset sizes from Kaggle to <code>data/raw/</code>.</p> <pre><code>uv run src/customer_support/data.py download\n</code></pre> <p>Or via invoke task:</p> <pre><code>uv run invoke download-data\n</code></pre> <p>This command: - Uses KaggleHub's caching mechanism (downloads only if not cached) - Saves all three dataset sizes (small, medium, full) as CSV files - Overwrites existing files in <code>data/raw/</code></p>"},{"location":"data/#preprocess-command","title":"Preprocess Command","text":"<p>Preprocesses datasets by cleaning, tokenizing, splitting, and saving as Parquet files.</p> <pre><code># Preprocess a specific dataset size\nuv run src/customer_support/data.py preprocess --dataset-type small\nuv run src/customer_support/data.py preprocess -d medium\n\n# Preprocess all dataset sizes\nuv run src/customer_support/data.py preprocess --all\n\n# Use a custom tokenizer\nuv run src/customer_support/data.py preprocess -d small -m bert-base-multilingual-cased\n</code></pre> <p>Or via invoke tasks:</p> <pre><code>uv run invoke preprocess-data --dataset-type small\nuv run invoke preprocess-data  # processes all sizes\n</code></pre> <p>Options: - <code>-d, --dataset-type</code>: Dataset size (<code>small</code>, <code>medium</code>, or <code>full</code>) - <code>-a, --all</code>: Process all dataset sizes - <code>-m, --model</code>: Tokenizer model name (default: <code>distilbert-base-multilingual-cased</code>)</p> <p>Note: The preprocess command always forces reprocessing, even if preprocessed data already exists.</p>"},{"location":"data/#python-api","title":"Python API","text":""},{"location":"data/#ticketdataset-class","title":"TicketDataset Class","text":"<p>The <code>TicketDataset</code> class provides a PyTorch-compatible dataset following the design patterns of <code>torchvision.datasets.MNIST</code>.</p>"},{"location":"data/#basic-usage","title":"Basic Usage","text":"<pre><code>from customer_support.data import TicketDataset\n\n# Load preprocessed data (most common use case)\ntrain_data = TicketDataset(root=\"data\", split=\"train\", dataset_type=\"small\")\n\n# Auto-download and preprocess if data not found\ntrain_data = TicketDataset(root=\"data\", split=\"train\", download=True)\n\n# Force reprocessing even if data exists\ntrain_data = TicketDataset(root=\"data\", split=\"train\", force_preprocess=True)\n\n# Load different splits\nval_data = TicketDataset(root=\"data\", split=\"validation\", dataset_type=\"small\")\ntest_data = TicketDataset(root=\"data\", split=\"test\", dataset_type=\"small\")\n</code></pre>"},{"location":"data/#with-dataloader","title":"With DataLoader","text":"<pre><code>from torch.utils.data import DataLoader\n\ntrain_data = TicketDataset(root=\"data\", split=\"train\", dataset_type=\"small\")\nloader = DataLoader(train_data, batch_size=32, shuffle=True)\n\nfor batch in loader:\n    input_ids = batch[\"input_ids\"]           # Shape: [batch_size, seq_length]\n    attention_mask = batch[\"attention_mask\"] # Shape: [batch_size, seq_length]\n    labels = batch[\"labels\"]                 # Shape: [batch_size]\n</code></pre>"},{"location":"data/#constructor-parameters","title":"Constructor Parameters","text":"<ul> <li><code>root</code> (str | Path): Root directory where dataset files are stored (e.g., <code>\"data\"</code>)</li> <li><code>split</code> (str): Dataset split - <code>\"train\"</code>, <code>\"validation\"</code>, or <code>\"test\"</code> (default: <code>\"train\"</code>)</li> <li><code>dataset_type</code> (str): Dataset size - <code>\"small\"</code>, <code>\"medium\"</code>, or <code>\"full\"</code> (default: <code>\"small\"</code>)</li> <li><code>download</code> (bool): If True, download and preprocess if not found (default: <code>False</code>)</li> <li><code>force_preprocess</code> (bool): If True, force reprocessing even if data exists (default: <code>False</code>)</li> <li><code>transform</code> (Callable | None): Optional transform applied to tokenized samples</li> <li><code>target_transform</code> (Callable | None): Optional transform applied to labels</li> <li><code>model_name</code> (str): Tokenizer model name (default: <code>\"distilbert-base-multilingual-cased\"</code>)</li> </ul>"},{"location":"data/#class-methods","title":"Class Methods","text":"<pre><code># Get the label mapping dictionary\nlabel_map = TicketDataset.get_label_map()\n# Returns: {\"very_low\": 0, \"low\": 1, \"medium\": 2, \"high\": 3, \"critical\": 4}\n</code></pre>"},{"location":"data/#return-format","title":"Return Format","text":"<p>Each sample returned by <code>__getitem__</code> is a dictionary with the following keys:</p> <ul> <li><code>input_ids</code> (torch.Tensor): Tokenized input IDs, shape <code>[seq_length]</code></li> <li><code>attention_mask</code> (torch.Tensor): Attention mask, shape <code>[seq_length]</code></li> <li><code>labels</code> (torch.Tensor): Priority label (0-4), shape <code>[]</code> (scalar)</li> </ul>"},{"location":"data/#custom-transforms","title":"Custom Transforms","text":"<pre><code># Apply custom transform to samples\ndef add_noise(sample):\n    sample[\"input_ids\"] = sample[\"input_ids\"] + torch.randn_like(sample[\"input_ids\"]) * 0.01\n    return sample\n\ndataset = TicketDataset(root=\"data\", split=\"train\", transform=add_noise)\n\n# Apply custom transform to labels\ndef offset_labels(label):\n    return label + 1\n\ndataset = TicketDataset(root=\"data\", split=\"train\", target_transform=offset_labels)\n</code></pre>"},{"location":"data/#file-structure","title":"File Structure","text":"<p>The data directory is organized as follows:</p> <pre><code>data/\n\u251c\u2500\u2500 raw/                          # Raw CSV files from Kaggle\n\u2502   \u251c\u2500\u2500 small.csv\n\u2502   \u251c\u2500\u2500 medium.csv\n\u2502   \u2514\u2500\u2500 full.csv\n\u2514\u2500\u2500 preprocessed/                 # Preprocessed Parquet files\n    \u251c\u2500\u2500 small_train.parquet\n    \u251c\u2500\u2500 small_validation.parquet\n    \u251c\u2500\u2500 small_test.parquet\n    \u251c\u2500\u2500 medium_train.parquet\n    \u251c\u2500\u2500 medium_validation.parquet\n    \u251c\u2500\u2500 medium_test.parquet\n    \u251c\u2500\u2500 full_train.parquet\n    \u251c\u2500\u2500 full_validation.parquet\n    \u2514\u2500\u2500 full_test.parquet\n</code></pre> <p>The Parquet format is used for efficient storage and fast loading of preprocessed datasets. Each split is saved as a separate file to enable selective loading.</p>"},{"location":"data/#data-versioning","title":"Data Versioning","text":"<p>The <code>data/</code> directory is tracked with DVC (Data Version Control) to maintain reproducibility across team members and environments.</p> <p>After preprocessing or modifying data, you should version your changes using DVC. See Data Version Control for the complete workflow on: - Setting up DVC with the DTU databar cluster - Versioning data changes with <code>dvc add</code>, tagging, and pushing - Pulling specific data versions - Troubleshooting DVC issues</p> <p>Quick reference:</p> <pre><code># After preprocessing, version the changes\ndvc add data/\ngit add data.dvc .gitignore\ngit commit -m \"Update preprocessed data\"\ngit tag -a \"v1.x\" -m \"data v1.x\"\ngit push --follow-tags\ndvc push\n</code></pre> <p>For detailed instructions and troubleshooting, refer to the Data Version Control documentation.</p>"},{"location":"dvc/","title":"Data Versioning","text":""},{"location":"dvc/#data-version-control","title":"Data Version Control","text":"<p>This project uses DVC for data versioning with Google Cloud Storage as the default remote. No credentials are required to pull data.</p> <p>The <code>./data/</code> directory is tracked with DVC and excluded from git to keep the repository lightweight while maintaining full data versioning capabilities.</p>"},{"location":"dvc/#initial-setup","title":"Initial setup","text":"<p>After cloning the repository, pull the data:</p> <pre><code>$ dvc pull\n</code></pre> <p>This will download the versioned data from the remote storage into your <code>./data/</code> directory.</p>"},{"location":"dvc/#versioning-and-contributing","title":"Versioning and contributing","text":"<p>By convention, data changes are tagged in the git history as follows:</p> <pre><code>git tag -a \"v1.0\" -m \"data v1.0\"\n</code></pre>"},{"location":"dvc/#making-changes-to-data","title":"Making changes to data","text":"<p>If changes are introduced to the data, follow this workflow:</p> <pre><code>$ dvc add data/           # Track changes to the data directory\n$ git add data.dvc .gitignore    # Stage DVC metadata files\n$ git commit -m \"&lt;change description&gt;\"\n$ git tag -a \"v1.x\" -m \"data v1.x\"  # Tag with an adjusted data version\n$ git push --follow-tags         # Push git commits and tags\n$ dvc push                # Push data to remote storage\n</code></pre> <p>Note: <code>uv run</code> might be necessary if project's virtual environment is not activated. Major version changes should be well-justified.</p>"},{"location":"dvc/#checking-data-status","title":"Checking data status","text":"<p>To see if your local data differs from what's tracked:</p> <pre><code>$ dvc status\n</code></pre>"},{"location":"dvc/#pulling-specific-data-versions","title":"Pulling specific data versions","text":"<p>To retrieve data from a specific version tag:</p> <pre><code>$ git checkout v1.0\n$ dvc pull\n</code></pre>"},{"location":"dvc/#troubleshooting","title":"Troubleshooting","text":""},{"location":"dvc/#data-conflicts","title":"Data conflicts","text":"<p>If <code>dvc pull</code> reports conflicts, your local changes may conflict with remote data:</p> <pre><code>$ dvc status              # Check what's different\n$ dvc checkout            # Discard local changes and match DVC-tracked version\n$ dvc pull                # Pull remote data\n</code></pre>"},{"location":"dvc/#deprecated-ssh-remote","title":"Deprecated: SSH Remote","text":"<p>The legacy SSH-based remote (<code>storage</code>) pointing to DTU's databar cluster is still available but deprecated.</p>"},{"location":"dvc/#prerequisites","title":"Prerequisites","text":"<ol> <li>Establish VPN connection with DTU (Cisco VPN - ITSwiki).</li> <li> <p>Create SSH keys for the transfer node (SSH - gbar.dtu.dk):</p> <p><code>bash $ ssh-keygen -t rsa $ ssh-copy-id -i ~/.ssh/id_rsa.pub transfer.gbar.dtu.dk $ ssh-copy-id -i ~/.ssh/id_rsa.pub sXXXXXX@transfer.gbar.dtu.dk</code> Replace <code>sXXXXXX</code> with your DTU student ID.</p> </li> <li> <p>Configure DVC to use the SSH remote:</p> <p><code>bash $ dvc remote default storage $ dvc remote modify --local storage user sXXXXXX</code></p> </li> </ol>"},{"location":"frontend/","title":"Frontend Service","text":"<p>The project includes a Streamlit web interface for interactive ticket priority classification with integration to the FastAPI backend.</p>"},{"location":"frontend/#overview","title":"Overview","text":"<p>The frontend is a user-friendly web application that classifies customer support tickets in real-time:</p> <ul> <li>Framework: Streamlit</li> <li>Backend: FastAPI (optional, with local model fallback)</li> <li>Port: 8501 (default)</li> <li>Model: DistilBERT with local inference support</li> </ul>"},{"location":"frontend/#quick-start","title":"Quick Start","text":""},{"location":"frontend/#running-locally","title":"Running Locally","text":"<p>Terminal 1: Start FastAPI Backend</p> <pre><code>uv run uvicorn customer_support.api:app --host 0.0.0.0 --port 8080 --reload\n</code></pre> <p>Terminal 2: Start Streamlit Frontend</p> <pre><code>uv run invoke frontend\n</code></pre> <p>Or directly:</p> <pre><code>uv run streamlit run src/customer_support/frontend.py\n</code></pre> <p>The frontend opens at <code>http://localhost:8501</code></p>"},{"location":"frontend/#architecture","title":"Architecture","text":"<pre><code>User Browser (Port 8501)\n    \u2193\nStreamlit UI\n    \u2193\n    \u251c\u2500\u2192 Try HTTP POST /predict to API (Port 8080)\n    \u2502\n    \u2514\u2500\u2192 Fallback: Local DistilBERT Model\n</code></pre>"},{"location":"frontend/#features","title":"Features","text":""},{"location":"frontend/#interactive-ui","title":"Interactive UI","text":"<ul> <li>Text Input: Paste or type customer support tickets</li> <li>Priority Classification: Displays predicted priority (Low, Medium, High)</li> <li>Confidence Score: Shows model confidence as percentage</li> <li>Prediction History: Tracks all predictions in current session</li> <li>Color-Coded Badges: Visual indicators for priority levels</li> <li>\ud83d\udfe2 Low (Green)</li> <li>\ud83d\udfe1 Medium (Orange)</li> <li>\ud83d\udd34 High (Red)</li> </ul>"},{"location":"frontend/#configuration","title":"Configuration","text":"<ul> <li>API URL Setting: Configure backend URL in sidebar</li> <li>Connection Testing: Built-in button to test API connectivity</li> <li>Environment Variables: Support for <code>API_URL</code> env var</li> </ul>"},{"location":"frontend/#inference-modes","title":"Inference Modes","text":"Mode Description When Used API (\ud83c\udf10) Calls FastAPI backend Primary mode, fastest Local (\ud83d\udda5\ufe0f) Uses local DistilBERT model API unavailable, offline Demo (\ud83c\udfaf) Keyword-based classification Model not found"},{"location":"frontend/#configuration_1","title":"Configuration","text":""},{"location":"frontend/#environment-variables","title":"Environment Variables","text":"<pre><code># Set custom API URL (default: http://localhost:8080)\nexport API_URL=http://api.example.com:8080\n\n# Set custom model path (default: models/model.ckpt)\nexport MODEL_PATH=/path/to/model.ckpt\n\n# Start frontend\nuv run invoke frontend\n</code></pre>"},{"location":"frontend/#sidebar-options","title":"Sidebar Options","text":"<ul> <li>FastAPI URL: Change API endpoint in real-time</li> <li>Check API Connection: Test if backend is reachable</li> <li>Shows current inference mode and API status</li> </ul>"},{"location":"frontend/#deployment-to-google-cloud","title":"Deployment to Google Cloud","text":""},{"location":"frontend/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Cloud Project set up</li> <li><code>gcloud</code> CLI installed and configured</li> <li>Artifact Registry API enabled</li> <li>Cloud Run API enabled</li> </ul>"},{"location":"frontend/#environment-setup","title":"Environment Setup","text":"<pre><code>export PROJECT_ID=\"your-gcp-project-id\"\nexport REGION=\"europe-west1\"\nexport REGISTRY=\"${REGION}-docker.pkg.dev\"\n</code></pre>"},{"location":"frontend/#create-artifact-registry-repository","title":"Create Artifact Registry Repository","text":"<pre><code>gcloud artifacts repositories create mlops-registry \\\n  --repository-format=docker \\\n  --location=$REGION \\\n  --project=$PROJECT_ID\n</code></pre>"},{"location":"frontend/#build-and-push-docker-image","title":"Build and Push Docker Image","text":"<p>Option A: Using Cloud Build (recommended for CI/CD)</p> <pre><code>gcloud builds submit \\\n  --config=cloudbuild-frontend.yaml \\\n  --project=$PROJECT_ID \\\n  --substitutions=_REGION=$REGION\n</code></pre> <p>Option B: Build Locally</p> <pre><code># Authenticate Docker\ngcloud auth configure-docker ${REGISTRY}\n\n# Build image\ndocker build -t ${REGISTRY}/${PROJECT_ID}/mlops-registry/frontend:latest \\\n  -f dockerfiles/frontend.dockerfile .\n\n# Push image\ndocker push ${REGISTRY}/${PROJECT_ID}/mlops-registry/frontend:latest\n</code></pre>"},{"location":"frontend/#deploy-to-cloud-run","title":"Deploy to Cloud Run","text":"<pre><code>gcloud run deploy customer-support-classifier \\\n  --image ${REGISTRY}/${PROJECT_ID}/mlops-registry/frontend:latest \\\n  --platform managed \\\n  --region $REGION \\\n  --project $PROJECT_ID \\\n  --memory 2Gi \\\n  --cpu 1 \\\n  --timeout 3600 \\\n  --allow-unauthenticated\n</code></pre>"},{"location":"frontend/#get-service-url","title":"Get Service URL","text":"<pre><code>gcloud run services describe customer-support-classifier \\\n  --region $REGION \\\n  --project $PROJECT_ID \\\n  --format='value(status.url)'\n</code></pre>"},{"location":"frontend/#deployment-configuration","title":"Deployment Configuration","text":""},{"location":"frontend/#environment-variables-on-cloud-run","title":"Environment Variables on Cloud Run","text":"<pre><code>gcloud run deploy customer-support-classifier \\\n  --set-env-vars MODEL_PATH=/app/models/model.ckpt \\\n  --set-env-vars API_URL=https://api-xxxxx.run.app \\\n  # ... other flags\n</code></pre> Variable Default Description <code>API_URL</code> <code>http://localhost:8080</code> FastAPI backend URL <code>MODEL_PATH</code> <code>models/model.ckpt</code> Local model checkpoint path <code>STREAMLIT_SERVER_PORT</code> <code>8080</code> Server port"},{"location":"frontend/#using-a-model-checkpoint","title":"Using a Model Checkpoint","text":"<p>1. Upload to Google Cloud Storage:</p> <pre><code>gsutil cp models/model.ckpt gs://${PROJECT_ID}-models/model.ckpt\n</code></pre> <p>2. Deploy with Cloud Storage path:</p> <pre><code>gcloud run deploy customer-support-classifier \\\n  --set-env-vars MODEL_PATH=/gcs/${PROJECT_ID}-models/model.ckpt \\\n  # ... other flags\n</code></pre>"},{"location":"frontend/#scaling","title":"Scaling","text":"<p>Adjust resources based on traffic:</p> <pre><code>gcloud run deploy customer-support-classifier \\\n  --memory 4Gi \\\n  --cpu 2 \\\n  --min-instances 1 \\\n  --max-instances 100 \\\n  # ... other flags\n</code></pre>"},{"location":"frontend/#resource-guidelines","title":"Resource Guidelines","text":"Load Memory CPU Min Instances Low 1Gi 1 0 Medium 2Gi 1 1 High 4Gi 2 2"},{"location":"frontend/#monitoring","title":"Monitoring","text":""},{"location":"frontend/#view-logs","title":"View Logs","text":"<pre><code>gcloud run logs read customer-support-classifier \\\n  --region $REGION \\\n  --project $PROJECT_ID\n</code></pre>"},{"location":"frontend/#view-metrics","title":"View Metrics","text":"<pre><code>gcloud monitoring dashboards list --project $PROJECT_ID\n</code></pre>"},{"location":"frontend/#demo-mode","title":"Demo Mode","text":"<p>If no model checkpoint is provided, the app runs in demo mode: - Shows a warning banner - Uses keyword-based priority classification - Perfect for testing UI without ML infrastructure</p>"},{"location":"frontend/#demo-keywords","title":"Demo Keywords","text":"<pre><code>High Priority: urgent, critical, emergency, down, broken, crash, fail\nMedium Priority: help, issue, problem, error, not working, unable\nLow Priority: question, info, fyi, documentation, feature request\n</code></pre>"},{"location":"frontend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"frontend/#app-fails-to-start","title":"App Fails to Start","text":"<ol> <li> <p>Check dependencies: <code>bash    uv run pip list | grep streamlit</code></p> </li> <li> <p>Check port availability: <code>bash    lsof -i :8501</code></p> </li> <li> <p>Increase memory if using model: <code>bash    gcloud run deploy ... --memory 4Gi</code></p> </li> </ol>"},{"location":"frontend/#api-connection-issues","title":"API Connection Issues","text":"<ol> <li> <p>Check API is running: <code>bash    curl http://localhost:8080/health</code></p> </li> <li> <p>Check API URL in sidebar - Default is <code>http://localhost:8080</code></p> </li> <li> <p>Check model checkpoint exists: <code>bash    ls models/model.ckpt</code></p> </li> <li> <p>Check firewall rules - May need to allow ports 8080 and 8501</p> </li> </ol>"},{"location":"frontend/#slow-predictions","title":"Slow Predictions","text":"<ul> <li>Increase timeout: API has 30-second timeout</li> <li>Check CPU/GPU usage: Monitor resource utilization</li> <li>Check model size: Larger models take longer to load</li> </ul>"},{"location":"frontend/#container-image-too-large","title":"Container Image Too Large","text":"<p>The Dockerfile uses multi-stage builds to minimize image size. Verify in logs.</p>"},{"location":"frontend/#performance-tips","title":"Performance Tips","text":"<ol> <li>API Response Time: ~100-500ms (depends on GPU/CPU)</li> <li>Streamlit Caching: Frontend caches responses to avoid duplicate calls</li> <li>Model Loading: Model loads once at startup (cached in memory)</li> <li>Batch Predictions: Consider batch API endpoint for many predictions</li> </ol>"},{"location":"frontend/#development-workflow","title":"Development Workflow","text":"<pre><code># 1. Make changes to frontend\nvim src/customer_support/frontend.py\n\n# 2. Streamlit auto-reloads on save (in demo mode)\n\n# 3. Test predictions\n\n# 4. Make model changes\nvim src/customer_support/model.py\n\n# 5. Retrain and update checkpoint\nuv run invoke train\n\n# 6. Restart API backend\n# (Ctrl+C in Terminal 1, then rerun)\n\n# 7. FastAPI auto-reloads with --reload flag\n</code></pre>"},{"location":"frontend/#docker-compose-local-testing","title":"Docker Compose (Local Testing)","text":"<pre><code>version: '3'\nservices:\n  api:\n    build:\n      context: .\n      dockerfile: dockerfiles/api.dockerfile\n    ports:\n      - \"8080:8080\"\n    environment:\n      - MODEL_PATH=/app/models/model.ckpt\n    volumes:\n      - ./models:/app/models\n\n  frontend:\n    build:\n      context: .\n      dockerfile: dockerfiles/frontend.dockerfile\n    ports:\n      - \"8501:8501\"\n    environment:\n      - API_URL=http://api:8080\n    depends_on:\n      - api\n</code></pre> <p>Run with:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"frontend/#cicd-integration","title":"CI/CD Integration","text":"<p>The <code>cloudbuild-frontend.yaml</code> automatically: 1. Builds Docker image on git push 2. Pushes to Artifact Registry 3. Deploys to Cloud Run</p> <p>Trigger builds:</p> <pre><code>gcloud builds triggers create github \\\n  --name=frontend-ci \\\n  --repo-name=mlops-project \\\n  --repo-owner=your-username \\\n  --branch-pattern=\"^main$\" \\\n  --build-config=cloudbuild-frontend.yaml\n</code></pre>"},{"location":"frontend/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Reduce Memory: Start with 1GB if model is small</li> <li>Min Instances 0: Auto-scaling with cold starts (~30s)</li> <li>Cloud Build Caching: Speeds up rebuilds</li> <li>Monitor Logs: Identify inefficiencies</li> </ul>"},{"location":"frontend/#next-steps","title":"Next Steps","text":"<ul> <li>Add batch prediction endpoint</li> <li>Implement analytics dashboard</li> <li>Add model versioning and A/B testing</li> <li>Set up monitoring and alerting</li> <li>Add authentication (OAuth, API keys)</li> </ul>"},{"location":"frontend/#related-documentation","title":"Related Documentation","text":"<ul> <li>API Service - FastAPI backend</li> <li>Model Architecture - DistilBERT implementation</li> <li>Cloud Deployment - GCP infrastructure</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide will help you set up the project for local development.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 - Check with <code>python --version</code></li> <li>UV - Modern Python package manager (install guide)</li> <li>Git - Version control</li> <li>Docker (optional) - For containerized training/inference</li> </ul> <p>For cloud features: - Google Cloud SDK - For DVC data access and Vertex AI training - Kaggle account - For downloading the dataset</p>"},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone &lt;repository-url&gt;\ncd mlops-project\n</code></pre>"},{"location":"getting_started/#2-install-dependencies","title":"2. Install dependencies","text":"<p>Using UV with CPU-only PyTorch (recommended for most development):</p> <pre><code>uv sync --extra cpu\n</code></pre> <p>For CUDA 12.8 GPU support:</p> <pre><code>uv sync --extra cu128\n</code></pre>"},{"location":"getting_started/#3-set-up-pre-commit-hooks","title":"3. Set up pre-commit hooks","text":"<pre><code>uv run pre-commit install\n</code></pre> <p>This enables automatic code quality checks (Ruff linting/formatting, MyPy type checking, Gitleaks secret scanning) before each commit.</p>"},{"location":"getting_started/#data-setup","title":"Data Setup","text":""},{"location":"getting_started/#option-a-pull-pre-processed-data-with-dvc-recommended","title":"Option A: Pull pre-processed data with DVC (recommended)","text":"<p>If you have GCP access configured:</p> <pre><code>uv run dvc pull\n</code></pre> <p>This downloads the pre-processed datasets from Google Cloud Storage.</p>"},{"location":"getting_started/#option-b-download-and-preprocess-from-kaggle","title":"Option B: Download and preprocess from Kaggle","text":"<ol> <li> <p>Set up Kaggle credentials (instructions)</p> </li> <li> <p>Download raw data:</p> </li> </ol> <pre><code>invoke download-data\n</code></pre> <ol> <li>Preprocess the data:</li> </ol> <pre><code># Process all dataset sizes (small, medium, full)\ninvoke preprocess-data\n\n# Or process a specific size\ninvoke preprocess-data --dataset-type medium\n</code></pre>"},{"location":"getting_started/#verify-setup","title":"Verify Setup","text":"<p>Run the test suite to verify everything is working:</p> <pre><code>invoke test\n</code></pre> <p>Run a quick training to test the full pipeline:</p> <pre><code>uv run python -m customer_support.train training=baseline dataset=small\n</code></pre>"},{"location":"getting_started/#environment-variables","title":"Environment Variables","text":"Variable Description Required <code>WANDB_API_KEY</code> Weights &amp; Biases API key for experiment tracking For training <code>KAGGLE_USERNAME</code> Kaggle username For data download <code>KAGGLE_KEY</code> Kaggle API key For data download <code>GOOGLE_APPLICATION_CREDENTIALS</code> Path to GCP service account JSON For DVC/cloud <p>To disable W&amp;B logging during local development:</p> <pre><code>export WANDB_MODE=disabled\n</code></pre>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Data Processing - Learn about the dataset and preprocessing pipeline</li> <li>Training - Train models locally or on the cloud</li> <li>CLI Commands - All available invoke tasks</li> </ul>"},{"location":"model/","title":"Model Architecture","text":"<p>This project uses a fine-tuned DistilBERT model for customer support ticket priority classification.</p>"},{"location":"model/#overview","title":"Overview","text":"<p>The model classifies customer support tickets into three priority levels:</p> Class ID Priority Description 0 Low Non-urgent tickets 1 Medium Standard priority 2 High Urgent tickets requiring immediate attention"},{"location":"model/#why-distilbert","title":"Why DistilBERT?","text":"<p>DistilBERT was chosen for several reasons:</p> <ul> <li>Multilingual support: Handles tickets in multiple languages (the dataset is multilingual)</li> <li>Efficiency: 40% smaller and 60% faster than BERT while retaining 97% of its performance</li> <li>Pre-trained: Leverages transfer learning from massive text corpora</li> <li>Production-ready: Suitable for real-time inference in production environments</li> </ul> <p>Model: <code>distilbert-base-multilingual-cased</code> - Parameters: ~66M (distilled from BERT's 110M) - Vocabulary: 119,547 tokens - Max sequence length: 512 tokens</p>"},{"location":"model/#ticketclassificationmodule","title":"TicketClassificationModule","text":"<p>The model is implemented as a PyTorch Lightning module in <code>src/customer_support/model.py</code>.</p>"},{"location":"model/#architecture","title":"Architecture","text":"<pre><code>Input Text\n    \u2193\nDistilBERT Tokenizer (512 max tokens)\n    \u2193\nDistilBERT Encoder (6 transformer layers)\n    \u2193\nClassification Head (Linear: 768 \u2192 3)\n    \u2193\nSoftmax\n    \u2193\nPriority Prediction (0, 1, or 2)\n</code></pre>"},{"location":"model/#key-features","title":"Key Features","text":"<ul> <li>PyTorch Lightning integration: Automatic training/validation/test loops</li> <li>Mixed precision training: 16-bit floating point for faster training</li> <li>Distributed training support: Scales across multiple GPUs</li> <li>TorchMetrics: Accurate metric computation with distributed sync</li> <li>Configurable LR scheduling: Optional warmup and decay</li> </ul>"},{"location":"model/#constructor-parameters","title":"Constructor Parameters","text":"<pre><code>TicketClassificationModule(\n    model_name=\"distilbert-base-multilingual-cased\",  # HuggingFace model ID\n    num_classes=3,                                     # Output classes\n    learning_rate=5e-5,                                # AdamW learning rate\n    weight_decay=0.01,                                 # L2 regularization\n    lr_scheduler_config=None,                          # Optional LR scheduler\n)\n</code></pre>"},{"location":"model/#usage","title":"Usage","text":"<pre><code>from customer_support.model import TicketClassificationModule\n\n# Initialize model\nmodel = TicketClassificationModule(learning_rate=5e-5)\n\n# Load from checkpoint\nmodel = TicketClassificationModule.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\n\n# Inference\nmodel.eval()\nwith torch.inference_mode():\n    outputs = model(input_ids=tokens, attention_mask=mask)\n    predictions = torch.argmax(outputs.logits, dim=-1)\n</code></pre>"},{"location":"model/#inputoutput-format","title":"Input/Output Format","text":""},{"location":"model/#input","title":"Input","text":"<p>The model expects tokenized input from the DistilBERT tokenizer:</p> Field Shape Description <code>input_ids</code> <code>[batch, seq_len]</code> Token IDs <code>attention_mask</code> <code>[batch, seq_len]</code> Attention mask (1=real, 0=padding) <code>labels</code> <code>[batch]</code> Ground truth labels (training only)"},{"location":"model/#output","title":"Output","text":"<p>The model returns a <code>SequenceClassifierOutput</code> with:</p> Field Shape Description <code>loss</code> <code>[]</code> Cross-entropy loss (if labels provided) <code>logits</code> <code>[batch, 3]</code> Raw logits for each class"},{"location":"model/#metrics","title":"Metrics","text":"<p>Training logs the following metrics to W&amp;B and CSV:</p> Metric Description <code>train_loss</code> Training loss (per step and epoch) <code>train_accuracy</code> Training accuracy (per epoch) <code>val_loss</code> Validation loss (per epoch) <code>val_accuracy</code> Validation accuracy (per epoch) <code>test_loss</code> Test loss (final evaluation) <code>test_accuracy</code> Test accuracy (final evaluation)"},{"location":"model/#checkpointing","title":"Checkpointing","text":"<p>Model checkpoints are saved automatically during training:</p> <ul> <li>Location: <code>outputs/YYYY-MM-DD/HH-MM-SS/checkpoints/</code></li> <li>Format: <code>epoch={epoch}-step={step}.ckpt</code></li> <li>Selection: Best model by <code>val_accuracy</code> (configurable)</li> <li>Contents: Model weights, optimizer state, hyperparameters</li> </ul>"},{"location":"model/#loading-checkpoints","title":"Loading Checkpoints","text":"<pre><code># For inference\nmodel = TicketClassificationModule.load_from_checkpoint(\n    \"checkpoints/epoch=5-step=1000.ckpt\",\n    local_files_only=True,  # Don't download model weights\n)\n\n# Resume training\ntrainer.fit(model, datamodule, ckpt_path=\"path/to/checkpoint.ckpt\")\n</code></pre>"},{"location":"model/#optimizer","title":"Optimizer","text":"<p>The model uses AdamW optimizer with:</p> <ul> <li>Learning rate: Configurable (default: 5e-5)</li> <li>Weight decay: 0.01 (L2 regularization)</li> <li>Optional LR scheduler: Cosine decay with warmup (production config)</li> </ul>"},{"location":"model/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training - How to train the model</li> <li>API - Using the model for inference</li> <li>Cloud - Training on Vertex AI</li> </ul>"},{"location":"overview/","title":"Welcome to <code>customer_support</code>","text":""},{"location":"overview/#overview","title":"Overview","text":"<p>Welcome to the documentation for <code>customer_support</code>, a machine learning project for classifying customer support ticket priorities using DistilBERT.</p>"},{"location":"overview/#project-architecture","title":"Project Architecture","text":"<p>The following diagram illustrates the architecture of the project: </p>"},{"location":"overview/#navigation","title":"Navigation","text":""},{"location":"overview/#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started: Set up your development environment and install dependencies. Learn about prerequisites and installation options for local development.</li> </ul>"},{"location":"overview/#core-documentation","title":"Core Documentation","text":"<ul> <li>Data: Explore the dataset structure, preprocessing steps, and data preparation pipeline for customer support tickets.</li> <li>Model: Understand the DistilBERT architecture, model design choices, and implementation details for ticket classification.</li> <li>Training: Learn how to train models locally with Hydra configuration management, hyperparameter tuning, and experiment tracking.</li> </ul>"},{"location":"overview/#deployment-infrastructure","title":"Deployment &amp; Infrastructure","text":"<ul> <li>Frontend: Deploy the Streamlit web interface with integrated FastAPI backend support, local fallback inference, and Google Cloud Run deployment guide.</li> <li>API: Deploy and use the FastAPI REST API for real-time ticket priority predictions in production.</li> <li>Cloud: Leverage Google Cloud Platform for scalable training jobs, CI/CD pipelines, and managed infrastructure with Vertex AI.</li> <li>DVC: Manage datasets and model artifacts using Data Version Control for reproducibility and collaboration.</li> </ul>"},{"location":"overview/#quality-assurance","title":"Quality Assurance","text":"<ul> <li>CLI: Use command-line tools to interact with the project for training, evaluation, and model management.</li> <li>Testing: Run the test suite, generate coverage reports, and understand the CI/CD testing strategy.</li> </ul>"},{"location":"testing/","title":"Testing","text":"<p>This project uses pytest for testing with coverage reporting and GitHub Actions for CI/CD.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":""},{"location":"testing/#quick-start","title":"Quick Start","text":"<pre><code># Run all tests with coverage\ninvoke test\n\n# Or directly with pytest\nuv run pytest tests/\n\n# With verbose output\nuv run pytest tests/ -v\n</code></pre>"},{"location":"testing/#coverage-report","title":"Coverage Report","text":"<pre><code># Run tests and generate coverage report\nuv run coverage run -m pytest tests/\nuv run coverage report -m\n\n# Generate HTML report\nuv run coverage html\n# Open htmlcov/index.html in browser\n</code></pre>"},{"location":"testing/#test-structure","title":"Test Structure","text":"<p>Tests are located in the <code>tests/</code> directory:</p> File Tests <code>test_data.py</code> Dataset loading, preprocessing, tokenization, splits <code>test_model.py</code> Model initialization, forward pass, Lightning integration <code>test_train.py</code> Training pipeline, callbacks, configuration <code>test_api.py</code> API endpoints, request validation, responses <code>test_evaluate.py</code> Model evaluation pipeline <code>test_visualize.py</code> Visualization generation"},{"location":"testing/#test-categories","title":"Test Categories","text":""},{"location":"testing/#data-tests-test_datapy","title":"Data Tests (<code>test_data.py</code>)","text":"<ul> <li>Dataset download and caching</li> <li>Preprocessing pipeline</li> <li>Tokenization correctness</li> <li>Train/val/test split ratios</li> <li>Label encoding</li> <li>Parquet file I/O</li> </ul>"},{"location":"testing/#model-tests-test_modelpy","title":"Model Tests (<code>test_model.py</code>)","text":"<ul> <li>Model instantiation</li> <li>Forward pass shapes</li> <li>Loss computation</li> <li>Metric computation</li> <li>Checkpoint save/load</li> </ul>"},{"location":"testing/#training-tests-test_trainpy","title":"Training Tests (<code>test_train.py</code>)","text":"<ul> <li>Trainer initialization</li> <li>Callback behavior</li> <li>Early stopping</li> <li>Checkpointing</li> <li>Hydra configuration loading</li> </ul>"},{"location":"testing/#api-tests-test_apipy","title":"API Tests (<code>test_api.py</code>)","text":"<ul> <li><code>GET /</code> endpoint</li> <li><code>GET /health</code> endpoint</li> <li><code>POST /predict</code> with valid input</li> <li>Input validation (empty text, missing fields)</li> <li>Response schema validation</li> </ul>"},{"location":"testing/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>Tests run automatically on GitHub Actions for every push and pull request to <code>main</code>.</p>"},{"location":"testing/#workflow-configuration","title":"Workflow Configuration","text":"<p>The workflow is defined in <code>.github/workflows/tests.yaml</code>:</p> <ul> <li>Triggers: Push to main, PRs to main, manual dispatch</li> <li>Platforms: Ubuntu, Windows, macOS</li> <li>Python: 3.12</li> <li>Coverage: Uploaded to Codecov</li> </ul>"},{"location":"testing/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>Checkout code</li> <li>Install UV and dependencies</li> <li>Run pytest with coverage</li> <li>Upload coverage report to Codecov</li> </ol>"},{"location":"testing/#concurrency","title":"Concurrency","text":"<p>The workflow uses concurrency groups to cancel outdated runs:</p> <pre><code>concurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n</code></pre>"},{"location":"testing/#writing-tests","title":"Writing Tests","text":""},{"location":"testing/#conventions","title":"Conventions","text":"<ul> <li>Test files: <code>test_&lt;module&gt;.py</code></li> <li>Test functions: <code>test_&lt;functionality&gt;()</code></li> <li>Use pytest fixtures for shared setup</li> <li>Mock external services (Kaggle, W&amp;B) where possible</li> </ul>"},{"location":"testing/#example-test","title":"Example Test","text":"<pre><code>import pytest\nfrom customer_support.model import TicketClassificationModule\n\ndef test_model_forward_pass():\n    model = TicketClassificationModule(num_classes=3)\n\n    # Create dummy input\n    input_ids = torch.randint(0, 1000, (2, 128))\n    attention_mask = torch.ones(2, 128)\n\n    # Forward pass\n    outputs = model(input_ids, attention_mask)\n\n    # Check output shape\n    assert outputs.logits.shape == (2, 3)\n</code></pre>"},{"location":"testing/#fixtures","title":"Fixtures","text":"<p>Common fixtures are defined in <code>tests/__init__.py</code> or individual test files:</p> <pre><code>@pytest.fixture\ndef sample_dataset():\n    \"\"\"Create a small test dataset.\"\"\"\n    return TicketDataset(root=\"data\", split=\"train\", dataset_type=\"small\")\n</code></pre>"},{"location":"testing/#code-quality","title":"Code Quality","text":"<p>In addition to tests, the project uses:</p> <ul> <li>Ruff: Linting and formatting</li> <li>MyPy: Type checking</li> <li>Pre-commit hooks: Run checks before each commit</li> </ul> <p>See Getting Started for setup instructions.</p>"},{"location":"training/","title":"Training","text":"<p>This guide covers how to train models locally. For cloud training on Vertex AI, see Cloud Infrastructure.</p>"},{"location":"training/#basic-training","title":"Basic Training","text":"<p>Run training with default configuration (medium dataset, baseline training):</p> <pre><code>uv run python -m customer_support.train\n</code></pre> <p>Or using invoke:</p> <pre><code>invoke train\n</code></pre>"},{"location":"training/#configuration-system","title":"Configuration System","text":"<p>Training uses Hydra for configuration management. Configs are located in <code>configs/</code> with this structure:</p> <pre><code>configs/\n\u251c\u2500\u2500 config.yaml          # Main config (defaults, paths, wandb)\n\u251c\u2500\u2500 dataset/\n\u2502   \u251c\u2500\u2500 small.yaml       # ~4k samples\n\u2502   \u251c\u2500\u2500 medium.yaml      # ~20k samples (default)\n\u2502   \u2514\u2500\u2500 full.yaml        # ~50k+ samples\n\u251c\u2500\u2500 training/\n\u2502   \u251c\u2500\u2500 baseline.yaml    # Quick experiments (default)\n\u2502   \u251c\u2500\u2500 production.yaml  # Production-quality training\n\u2502   \u2514\u2500\u2500 best_sweep.yaml  # Optimized hyperparameters\n\u2514\u2500\u2500 experiment/\n    \u251c\u2500\u2500 baseline.yaml    # Baseline experiment preset\n    \u251c\u2500\u2500 production.yaml  # Production experiment preset\n    \u2514\u2500\u2500 best.yaml        # Best hyperparameters from sweep\n</code></pre>"},{"location":"training/#configuration-options","title":"Configuration Options","text":""},{"location":"training/#dataset-selection","title":"Dataset Selection","text":"<pre><code># Small dataset for quick tests\nuv run python -m customer_support.train dataset=small\n\n# Full dataset for production\nuv run python -m customer_support.train dataset=full\n</code></pre>"},{"location":"training/#training-configurations","title":"Training Configurations","text":"<p>Baseline (default) - Fast experiments:</p> <pre><code>uv run python -m customer_support.train training=baseline\n</code></pre> <ul> <li>5 epochs, batch size 64, learning rate 1e-4</li> <li>Early stopping patience: 3</li> </ul> <p>Production - Full training:</p> <pre><code>uv run python -m customer_support.train training=production\n</code></pre> <ul> <li>10 epochs, batch size 32, learning rate 5e-5</li> <li>Cosine LR scheduler with warmup</li> <li>Gradient clipping, gradient accumulation</li> <li>Deterministic training</li> </ul>"},{"location":"training/#experiment-presets","title":"Experiment Presets","text":"<p>Use experiment configs for predefined combinations:</p> <pre><code># Best hyperparameters from sweep + full dataset\nuv run python -m customer_support.train experiment=best\n</code></pre>"},{"location":"training/#overriding-parameters","title":"Overriding Parameters","text":"<p>Override any parameter from command line:</p> <pre><code>uv run python -m customer_support.train \\\n    dataset=medium \\\n    training.batch_size=32 \\\n    training.learning_rate=5e-5 \\\n    training.num_epochs=10\n</code></pre>"},{"location":"training/#hardware-configuration","title":"Hardware Configuration","text":"<pre><code># Force CPU\nuv run python -m customer_support.train accelerator=cpu\n\n# Specific GPU\nuv run python -m customer_support.train devices=1 accelerator=gpu\n</code></pre>"},{"location":"training/#experiment-tracking","title":"Experiment Tracking","text":"<p>Training is automatically logged to Weights &amp; Biases (if <code>WANDB_API_KEY</code> is set).</p>"},{"location":"training/#wb-configuration","title":"W&amp;B Configuration","text":"<p>In <code>configs/config.yaml</code>:</p> <pre><code>wandb:\n  entity: \"dtu-mlops-g41\"\n  project: \"customer_support\"\n  mode: \"online\"       # online, offline, disabled\n  log_model: true      # Log model checkpoints\n</code></pre>"},{"location":"training/#disable-wb-logging","title":"Disable W&amp;B Logging","text":"<pre><code># Via environment variable\nexport WANDB_MODE=disabled\n\n# Or via config override\nuv run python -m customer_support.train wandb.mode=disabled\n</code></pre>"},{"location":"training/#training-output","title":"Training Output","text":"<p>Training creates output directories under <code>outputs/YYYY-MM-DD/HH-MM-SS/</code>:</p> <pre><code>outputs/2024-01-15/10-30-00/\n\u251c\u2500\u2500 .hydra/              # Hydra config snapshots\n\u251c\u2500\u2500 checkpoints/         # Model checkpoints\n\u2502   \u2514\u2500\u2500 epoch=X-step=Y.ckpt\n\u251c\u2500\u2500 csv_logs/            # Training metrics CSV\n\u2514\u2500\u2500 train.log            # Training logs\n</code></pre>"},{"location":"training/#callbacks","title":"Callbacks","text":""},{"location":"training/#early-stopping","title":"Early Stopping","text":"<p>Stops training when validation accuracy stops improving: - Monitor: <code>val_accuracy</code> - Patience: configurable (default: 3 epochs)</p>"},{"location":"training/#model-checkpointing","title":"Model Checkpointing","text":"<p>Saves best model(s) based on validation accuracy: - Monitor: <code>val_accuracy</code> - Mode: <code>max</code> - Save top K: configurable (default: 1)</p>"},{"location":"training/#key-training-parameters","title":"Key Training Parameters","text":"Parameter Baseline Production Description <code>batch_size</code> 64 32 Batch size per device <code>learning_rate</code> 1e-4 5e-5 Initial learning rate <code>num_epochs</code> 5 10 Maximum epochs <code>patience</code> 3 5 Early stopping patience <code>precision</code> 16-mixed 16-mixed Mixed precision training <code>gradient_clip_val</code> null 1.0 Gradient clipping <code>accumulate_grad_batches</code> 1 2 Gradient accumulation"},{"location":"training/#example-full-training-run","title":"Example: Full Training Run","text":"<pre><code># Production training on full dataset with W&amp;B logging\nexport WANDB_API_KEY=your_key_here\n\nuv run python -m customer_support.train \\\n    experiment=best \\\n    seed=42\n</code></pre>"}]}